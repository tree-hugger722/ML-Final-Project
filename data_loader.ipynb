{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43f4a8ca",
   "metadata": {},
   "source": [
    "Loads in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "484eadb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "import calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f21fe2-2f89-41c6-a8a4-c938fb5b6aa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4452048-3c15-42eb-9ad4-47e71729b091",
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants in all caps\n",
    "DEFAULT_ENERGY_MEDIAN = 89.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c44bd6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_energy_data() -> Tuple[Dict[str, pd.DataFrame], pd.DataFrame, pd.DataFrame]:\n",
    "    '''\n",
    "    Loads each of the 5 data sets (3 sets of meter readings, 1 meta data set, and 1 median energy usage data set):\n",
    "    \n",
    "    Returns:\n",
    "    ---------\n",
    "    meter_data: {'meter_type':pd.DataFrame}\n",
    "    - Much easier this way to keep everything compact & organized within a single variable.\n",
    "    \n",
    "    metadata: pd.DataFrame\n",
    "    \n",
    "    median_energy_usage: pd.DataFrame\n",
    "    \n",
    "    '''\n",
    "    # read in data from meter readings\n",
    "    source_dir = \"building_genome_datasets/\"\n",
    "    file_suffix = \"_cleaned.csv\"\n",
    "    \n",
    "    # meters = [\"electricity\",\"gas\",\"solar\"]\n",
    "    \n",
    "    meter_data = {}\n",
    "    \n",
    "    ### We can also save some repetetive logic by leveraging the way the files are already named\n",
    "    for dataset in os.listdir(path=source_dir):\n",
    "        if '_cleaned.csv' in dataset:\n",
    "            meter_type = dataset.split('_')[0]\n",
    "            data = pd.read_csv(source_dir+dataset)\n",
    "            meter_data[meter_type] = data\n",
    "    \n",
    "    ### OLD\n",
    "    # electricity_data = pd.read_csv(source_dir+\"electricity\"+file_suffix)\n",
    "    # gas_data = pd.read_csv(source_dir+\"gas\"+file_suffix)\n",
    "    # solar_data = pd.read_csv(source_dir+\"solar\"+file_suffix)\n",
    "    \n",
    "    metadata = pd.read_csv(source_dir+\"metadata.csv\")\n",
    "    metadata = metadata.drop([\"site_id\", \"building_id_kaggle\", \"site_id_kaggle\", \"eui\", \"heatingtype\", \"source_eui\", \"site_eui\", \"energystarscore\", \"leed_level\", \"rating\", \"date_opened\", \"electricity\", \"hotwater\", \"chilledwater\", \"solar\", \"gas\", \"steam\", \"water\", \"irrigation\", \"industry\", \"subindustry\"], axis=1)\n",
    "    # median_energy_usage = pd.read_csv(\"CS374-Final-Project/median_energy_usage.csv\")\n",
    "    median_energy_usage = pd.read_csv(\"median_energy_usage.csv\")\n",
    "\n",
    "    return meter_data, metadata, median_energy_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f251c89c",
   "metadata": {},
   "source": [
    "Source EUI calculation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5abbf40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_source_eui(area, total_energy):\n",
    "    \"\"\"\n",
    "    Given an int total_energy in kWh and an int area representing total building surface area,\n",
    "    returns the source energy use intensity\n",
    "    \"\"\"\n",
    "    return total_energy/area\n",
    "\n",
    "def calculate_total_energy(electricity, gas, solar):\n",
    "    \"\"\"\n",
    "    Given electricity, natural gas, and solar meter reading in kWh returns the total energy \n",
    "    usage (weighted according to the American Insistute of Architects:\n",
    "    https://aiacalifornia.org/energy-use-intensity-eui/)\n",
    "    \"\"\"\n",
    "    return (electricity*2.8)+(gas*1.05)+solar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dc9cc2",
   "metadata": {},
   "source": [
    "Helper functions for adding categorical variables and generating the outcome variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8cd75e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_categorical(data, category):\n",
    "    \"\"\"\n",
    "    Given a dataframe data and string category, returns a dataframe with \n",
    "    category converted into categorical variables with 1/0 encoding\n",
    "    \"\"\"\n",
    "    data_copy = data.copy()\n",
    "    data_copy = pd.get_dummies(data_copy, prefix= [category], columns=[category])\n",
    "    return data_copy\n",
    "\n",
    "def add_eui_and_outcome(electricity, gas, solar, data, median_energy_usage):\n",
    "    \"\"\"\n",
    "    Given the aggregate dataset, electricity, gas, and solar meter data sets and\n",
    "    the reference data set of national median energy use intensity for each\n",
    "    type of building, calculates the source's eui, adds it to the aggregate \n",
    "    dataset and adds a binary outcome that is 1 if the source eui is < the\n",
    "    national median and 0 otherwise.\n",
    "    \"\"\"\n",
    "    data_copy = data.copy()\n",
    "    source_euis = []\n",
    "    isEfficent = []\n",
    "\n",
    "    electricity_start = electricity.index[electricity['timestamp'] == \"2017-01-01 00:00:00\"].tolist()\n",
    "    electricity_end = electricity.index[electricity['timestamp'] == \"2017-12-31 23:00:00\"].tolist()\n",
    "    gas_start = gas.index[gas['timestamp'] == \"2017-01-01 00:00:00\"].tolist()\n",
    "    gas_end = gas.index[gas['timestamp'] == \"2017-12-31 23:00:00\"].tolist()\n",
    "    solar_start = solar.index[solar['timestamp'] == \"2017-01-01 00:00:00\"].tolist()\n",
    "    solar_end = solar.index[solar['timestamp'] == \"2017-12-31 23:00:00\"].tolist()\n",
    "\n",
    "    for row in range(len(data)):\n",
    "        # get the building id\n",
    "        building_id = data_copy['building_id'].iloc[row]\n",
    "\n",
    "        # sum electricity usage for this building in 2017\n",
    "        electricity_total = 0\n",
    "        gas_total = 0\n",
    "        solar_total = 0\n",
    "\n",
    "        # check if building is in the electricity meter set\n",
    "        if building_id in electricity.columns.tolist():\n",
    "            new_electricity = electricity.copy()           \n",
    "            electricity_total_building = new_electricity.get(building_id)\n",
    "            electricity_total = (electricity_total_building.iloc[electricity_start[0]:electricity_end[0]]).sum()\n",
    "        if building_id in gas.columns.tolist():\n",
    "            new_gas = gas.copy()\n",
    "            gas_total_building = new_gas.get(building_id)\n",
    "            gas_total = (gas_total_building.iloc[gas_start[0]:gas_end[0]]).sum()\n",
    "        if building_id in solar.columns.tolist():\n",
    "            new_solar = solar.copy()\n",
    "            solar_total_building = new_solar.get(building_id)\n",
    "            solar_total = (solar_total_building.iloc[solar_start[0]:solar_end[0]]).sum()\n",
    "\n",
    "        total_energy = calculate_total_energy(electricity_total, gas_total, solar_total)\n",
    "        source_eui = calculate_source_eui(data_copy['sqft'].iloc[row], total_energy)\n",
    "        source_euis.append(source_eui)\n",
    "\n",
    "        comparison_val = get_median_energy_val(median_energy_usage, get_primary_cat(data_copy, row), get_sub_cat(data_copy,row))\n",
    "        \n",
    "        if comparison_val > source_eui:\n",
    "            isEfficent.append(1)\n",
    "        else:\n",
    "            isEfficent.append(0)\n",
    "\n",
    "    data_copy[\"source_eui_est\"] = source_euis\n",
    "    data_copy[\"isEfficient\"] = isEfficent\n",
    "\n",
    "    return data_copy\n",
    "\n",
    "def get_primary_cat(data, row):\n",
    "    \"\"\"\n",
    "    Retrieves the string primaryspaceusage of the building in a given row of data\n",
    "    \"\"\"\n",
    "    new_df = data.copy()\n",
    "    new_df = new_df.filter(regex=\"^primaryspaceusage\",axis=1)\n",
    "    for col in new_df.columns.tolist():\n",
    "        if new_df.get(col).iloc[row] == 1:\n",
    "            return col\n",
    "    return \"other\"\n",
    "\n",
    "def get_sub_cat(data, row):\n",
    "    \"\"\"\n",
    "    Retrieves the string sub_primaryspaceusage of the building in a given row of data\n",
    "    \"\"\"\n",
    "    new_df = data.copy()\n",
    "    new_df = new_df.filter(regex=\"^sub_primaryspaceusage\",axis=1)\n",
    "    for col in new_df.columns.tolist():\n",
    "        if new_df.get(col).iloc[row] == 1:\n",
    "            return col\n",
    "    return \"other\"\n",
    "\n",
    "def get_median_energy_val(median_energy_usage, primary_cat, sub_cat):\n",
    "    \"\"\"\n",
    "    Given the reference set of median energy use intensity by building category,\n",
    "    the primary and sub usage categories, returns the corresponding median\n",
    "    source eui. \n",
    "    \"\"\"\n",
    "    primary_cat = primary_cat.split(\"_\")[-1].lower()\n",
    "    sub_cat = sub_cat.split(\"_\")[-1].lower()\n",
    "    broad_cat_indices = median_energy_usage.index[median_energy_usage['Broad Category'] == primary_cat].tolist()\n",
    "    broad_cat_df = median_energy_usage.iloc[broad_cat_indices]\n",
    "\n",
    "    \n",
    "    # iterate through broad_cat_df and check if name of the sub_cat is the same as either the Primary Function \n",
    "    # or the Further Breakdown categories\n",
    "    for row in range(len(broad_cat_df)):\n",
    "\n",
    "        if broad_cat_df['Primary Function'].iloc[row] == sub_cat:\n",
    "            return broad_cat_df[\"Source EUI\"].iloc[row]\n",
    "\n",
    "        if broad_cat_df['Further Breakdown'].iloc[row] != \"\":\n",
    "            if sub_cat == broad_cat_df['Further Breakdown'].iloc[row]:\n",
    "                return broad_cat_df['Source EUI'].iloc[row]\n",
    "\n",
    "    # find the corresponding 'Other' Category (this is simplified for now, \n",
    "    # improve this to check the 'Further Breakdown' for the future)\n",
    "    for row in range(len(broad_cat_df)):\n",
    "        if (sub_cat in broad_cat_df['Primary Function'].iloc[row]) or (\"other\" in broad_cat_df['Primary Function'].iloc[row]):\n",
    "            return broad_cat_df[\"Source EUI\"].iloc[row]   \n",
    "        \n",
    "    #else return default value\n",
    "    return DEFAULT_ENERGY_MEDIAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe08a90",
   "metadata": {},
   "source": [
    "Helper functions for adding processed meter readings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b425f388-5d40-4b34-980a-b163b9832dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_month_numbers() -> dict:\n",
    "    \"\"\"\n",
    "    Helper function for generating the following: \n",
    "    \n",
    "        {\n",
    "         'January': '01',\n",
    "         'February': '02',\n",
    "         'March': '03',\n",
    "         'April': '04',\n",
    "         'May': '05',\n",
    "         'June': '06',\n",
    "         'July': '07',\n",
    "         'August': '08',\n",
    "         'September': '09',\n",
    "         'October': '10',\n",
    "         'November': '11',\n",
    "         'December': '12'\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    month_names = list(calendar.month_name)[1:]\n",
    "    month_nums = [pd.to_datetime(mn, format='%B').strftime('%m') for mn in month_names]\n",
    "    \n",
    "    return dict(zip(month_names, month_nums))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc6c86c-6c71-49cd-9bde-49802d4a5f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_daily_totals(start, end, data):\n",
    "    \"\"\"\n",
    "    Given some dataframe containing meter readings, a start and an end timestamp,\n",
    "    returns a dataframe with a single row containing the sum of all the readings\n",
    "    in that timeframe\n",
    "    \"\"\"\n",
    "    data_copy = data.copy()\n",
    "    start_index = data_copy.index[data_copy['timestamp'] == start].tolist()\n",
    "    end_index = data_copy.index[data_copy['timestamp'] == end].tolist()\n",
    "    column_sum = data_copy.iloc[start_index[0]:end_index[0]].sum().to_frame().T\n",
    "\n",
    "    return column_sum\n",
    "\n",
    "def avg_daily_totals(daily_totals, month, meter_type):\n",
    "    \"\"\"\n",
    "    Given a list of dataframes containing singular rows with total daily resource usage (each \n",
    "    dataframe in the list holds totals from a particular day in the week), the number of buildings in the\n",
    "    dataset, the string month and string meter type, returns a single dataframe containing\n",
    "    the average daily resource usage over the week\n",
    "    \"\"\"\n",
    "\n",
    "    data = daily_totals[0].copy()\n",
    "    data = data.iloc[:,1:len(data.columns.tolist())]\n",
    "    for day in range(1,len(daily_totals)):\n",
    "        adjusted_daily_total_frame = daily_totals[day].copy().iloc[:,1:len(daily_totals[day].columns.tolist())]\n",
    "        data = data.combine(adjusted_daily_total_frame, lambda x,y: x+y, fill_value=0)\n",
    "\n",
    "    daily_avg_over_week = data.apply(lambda x: x/7, raw=True, axis=1)\n",
    "\n",
    "    daily_avg_over_week = daily_avg_over_week.transpose().reset_index().rename(columns={'index':'building_id'})\n",
    "    daily_avg_over_week.rename(columns={0:(meter_type+\"_\"+month)}, inplace=True)\n",
    "\n",
    "    return daily_avg_over_week\n",
    "\n",
    "def avg_meter_readings(meter_set, meter_type, month):\n",
    "    \"\"\"\n",
    "    Given a dataset for a particular meter's readings, the number of buildings\n",
    "    in the data set, and the month of readings to be analyzed, returns a dataframe\n",
    "    of daily averages for a week in that month.\n",
    "    \"\"\"\n",
    "    \n",
    "    month_to_num = get_month_numbers()\n",
    "    month_number = month_to_num[month]\n",
    "    \n",
    "    # if month == \"jan\":\n",
    "    #     mon = \"01\"\n",
    "    # elif month == \"april\":\n",
    "    #     mon = \"04\"\n",
    "    # elif month == \"july\":\n",
    "    #     mon = \"07\"\n",
    "    # elif month == \"oct\":\n",
    "    #     mon = \"10\"\n",
    "    # else:\n",
    "    #     raise Exception(\"Invalid month\")\n",
    "    \n",
    "\n",
    "    # date_bounds = [(\"2016-\"+mon+\"-01 00:00:00\", \"2016-\"+mon+\"-02 00:00:00\"),(\"2016-\"+mon+\"-02 00:00:00\", \"2016-\"+mon+\"-03 00:00:00\"),(\"2016-\"+mon+\"-03 00:00:00\", \"2016-\"+mon+\"-04 00:00:00\"),(\"2016-\"+mon+\"-04 00:00:00\", \"2016-\"+mon+\"-05 00:00:00\"),(\"2016-\"+mon+\"-05 00:00:00\", \"2016-\"+mon+\"-06 00:00:00\"),(\"2016-\"+mon+\"-06 00:00:00\", \"2016-\"+mon+\"-07 00:00:00\"),(\"2016-\"+mon+\"-07 00:00:00\", \"2016-\"+mon+\"-08 00:00:00\")]\n",
    "    # get rid of repetetive code with a loop:\n",
    "    date_bounds = []\n",
    "    for i in range(1, 9):\n",
    "        start_date = \"2016-\" + month_number + \"-\" + str(i).zfill(2) + \" 00:00:00\"\n",
    "        end_date = \"2016-\" + month_number + \"-\" + str(i + 1).zfill(2) + \" 00:00:00\"\n",
    "        date_bounds.append((start_date, end_date))\n",
    "    \n",
    "    daily_totals = [find_daily_totals(start,end,meter_set) for (start,end) in date_bounds]\n",
    "\n",
    "\n",
    "    # now average each of the values from the sets and output a new dataframe with readings from each building\n",
    "    return avg_daily_totals(daily_totals, month, meter_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288cbcef-86e6-4c50-be7c-909ef594cb59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e1a72c1-e09a-4e54-9cfb-fbfb5514d3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2016-01-01 00:00:00', '2016-01-02 00:00:00'),\n",
       " ('2016-01-02 00:00:00', '2016-01-03 00:00:00'),\n",
       " ('2016-01-03 00:00:00', '2016-01-04 00:00:00'),\n",
       " ('2016-01-04 00:00:00', '2016-01-05 00:00:00'),\n",
       " ('2016-01-05 00:00:00', '2016-01-06 00:00:00'),\n",
       " ('2016-01-06 00:00:00', '2016-01-07 00:00:00'),\n",
       " ('2016-01-07 00:00:00', '2016-01-08 00:00:00')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mon = '01'\n",
    "\n",
    "[(\"2016-\"+mon+\"-01 00:00:00\", \"2016-\"+mon+\"-02 00:00:00\"),(\"2016-\"+mon+\"-02 00:00:00\", \"2016-\"+mon+\"-03 00:00:00\"),(\"2016-\"+mon+\"-03 00:00:00\", \"2016-\"+mon+\"-04 00:00:00\"),(\"2016-\"+mon+\"-04 00:00:00\", \"2016-\"+mon+\"-05 00:00:00\"),(\"2016-\"+mon+\"-05 00:00:00\", \"2016-\"+mon+\"-06 00:00:00\"),(\"2016-\"+mon+\"-06 00:00:00\", \"2016-\"+mon+\"-07 00:00:00\"),(\"2016-\"+mon+\"-07 00:00:00\", \"2016-\"+mon+\"-08 00:00:00\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7abe38-9974-4ac3-8a22-1c4ded436d34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e817732f-888c-4747-a3f0-693b9772f7f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ac493cc-83e2-4d9e-9c6a-22cb78a6936b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "from functools import wraps\n",
    "\n",
    "\n",
    "###EXAMPLE OF CODE PROFILING\n",
    "def profile(output_file=None, sort_by='cumulative', lines_to_print=50, strip_dirs=False):\n",
    "    \"\"\"A time profiler decorator.\n",
    "    Inspired by and modified the profile decorator of Giampaolo Rodola:\n",
    "    http://code.activestate.com/recipes/577817-profile-decorator/\n",
    "    Args:\n",
    "        output_file: str or None. Default is None\n",
    "            Path of the output file. If only name of the file is given, it's\n",
    "            saved in the current directory.\n",
    "            If it's None, the name of the decorated function is used.\n",
    "        sort_by: str or SortKey enum or tuple/list of str/SortKey enum\n",
    "            Sorting criteria for the Stats object.\n",
    "            For a list of valid string and SortKey refer to:\n",
    "            https://docs.python.org/3/library/profile.html#pstats.Stats.sort_stats\n",
    "        lines_to_print: int or None\n",
    "            Number of lines to print. Default (None) is for all the lines.\n",
    "            This is useful in reducing the size of the printout, especially\n",
    "            that sorting by 'cumulative', the time consuming operations\n",
    "            are printed toward the top of the file.\n",
    "        strip_dirs: bool\n",
    "            Whether to remove the leading path info from file names.\n",
    "            This is also useful in reducing the size of the printout\n",
    "    Returns:\n",
    "        Profile of the decorated function\n",
    "    \"\"\"\n",
    "\n",
    "    def inner(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            _output_file = output_file or func.__name__ + '.prof'\n",
    "            pr = cProfile.Profile()\n",
    "            pr.enable()\n",
    "            retval = func(*args, **kwargs)\n",
    "            pr.disable()\n",
    "            pr.dump_stats(_output_file)\n",
    "\n",
    "            with open(_output_file, 'w') as f:\n",
    "                ps = pstats.Stats(pr, stream=f)\n",
    "                if strip_dirs:\n",
    "                    ps.strip_dirs()\n",
    "                if isinstance(sort_by, (tuple, list)):\n",
    "                    ps.sort_stats(*sort_by)\n",
    "                else:\n",
    "                    ps.sort_stats(sort_by)\n",
    "                ps.print_stats(lines_to_print)\n",
    "            return retval\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109b0f92",
   "metadata": {},
   "source": [
    "Top-level function to clean the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d6ec679",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Decorating function with profiler\n",
    "@profile(sort_by='cumulative', lines_to_print=50, strip_dirs=True)\n",
    "def clean_data(meter_data, metadata, median_energy_usage, months):\n",
    "    \"\"\"\n",
    "    Calculates the average energy use over a week in \n",
    "    each season for each meter, calculates the source eui for 2017 of each\n",
    "    building, converts categorical columns into dummy variables, creates a \n",
    "    binary outcome variable (1 if building's 2017 source eui is above the \n",
    "    national median, 0 otherwise) and returns the aggregated dataset.\n",
    "    \"\"\"\n",
    "    # create 32 new features: one daily average reading over a week per season, per meter type\n",
    "    # for meter_type in range(len(meters)):\n",
    "    for meter_type, meter_set in meter_data.items():\n",
    "    \n",
    "        for month in months:\n",
    "            # new_feature = avg_meter_readings(meter_sets[meter_type], meters[meter_type], month)\n",
    "            new_feature = avg_meter_readings(meter_set, meter_type, month)\n",
    "\n",
    "            # join in a way that aligns the building_id\n",
    "            metadata = pd.merge(metadata, new_feature, on=\"building_id\", how=\"left\")\n",
    "     \n",
    "    metadata = convert_categorical(metadata, \"primaryspaceusage\")\n",
    "    metadata = convert_categorical(metadata, \"sub_primaryspaceusage\")\n",
    "    metadata = convert_categorical(metadata, \"timezone\")\n",
    "\n",
    "    # metadata = add_eui_and_outcome(meter_sets[meters.index(\"electricity\")], meter_sets[meters.index(\"gas\")], meter_sets[meters.index(\"solar\")], metadata, median_energy_usage)\n",
    "    metadata = add_eui_and_outcome(meter_data[\"electricity\"], meter_data[\"gas\"], meter_data[\"gas\"], metadata, median_energy_usage)\n",
    "    metadata.to_csv(\"building_genome_datasets/final3_cleaned.csv\")\n",
    "\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206f831d",
   "metadata": {},
   "source": [
    "Generate the final data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26740efd-cd9a-465b-a42d-6ee90b756540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final data set in final_data\n",
    "meter_data, metadata, median_energy_usage = load_energy_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08401f3a-7120-4f6a-b954-3d868892331d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Showing an example of using pandas for sampling (much faster when timestamp is the index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4c4c9431-36c1-48fa-b449-6f83dcd15450",
   "metadata": {},
   "outputs": [],
   "source": [
    "electricity_data = meter_data['electricity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "027df587-c329-41a7-b53e-1745770ddfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "electricity_data['timestamp']=pd.to_datetime(electricity_data['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "285ac762-c748-453c-ad05-4c99ad36ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = electricity_data.set_index('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5accbaeb-5ba1-48b1-8667-094229589bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Panther_parking_Lorriane</th>\n",
       "      <th>Panther_lodging_Cora</th>\n",
       "      <th>Panther_office_Hannah</th>\n",
       "      <th>Panther_lodging_Hattie</th>\n",
       "      <th>Panther_education_Teofila</th>\n",
       "      <th>Panther_education_Jerome</th>\n",
       "      <th>Panther_retail_Felix</th>\n",
       "      <th>Panther_parking_Asia</th>\n",
       "      <th>Panther_education_Misty</th>\n",
       "      <th>Panther_retail_Gilbert</th>\n",
       "      <th>...</th>\n",
       "      <th>Cockatoo_public_Caleb</th>\n",
       "      <th>Cockatoo_education_Tyler</th>\n",
       "      <th>Cockatoo_public_Shad</th>\n",
       "      <th>Mouse_health_Buddy</th>\n",
       "      <th>Mouse_health_Modesto</th>\n",
       "      <th>Mouse_lodging_Vicente</th>\n",
       "      <th>Mouse_health_Justin</th>\n",
       "      <th>Mouse_health_Ileana</th>\n",
       "      <th>Mouse_health_Estela</th>\n",
       "      <th>Mouse_science_Micheal</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>21.797071</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.809000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.652859</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>191.925676</td>\n",
       "      <td>790.725203</td>\n",
       "      <td>143.564155</td>\n",
       "      <td>17.633042</td>\n",
       "      <td>891.485559</td>\n",
       "      <td>44.260756</td>\n",
       "      <td>636.270732</td>\n",
       "      <td>67.270160</td>\n",
       "      <td>388.334665</td>\n",
       "      <td>336.222492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-29</th>\n",
       "      <td>2.952560</td>\n",
       "      <td>22.008425</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.163100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.227683</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>230.411836</td>\n",
       "      <td>813.343694</td>\n",
       "      <td>152.451438</td>\n",
       "      <td>17.644900</td>\n",
       "      <td>800.878622</td>\n",
       "      <td>45.804598</td>\n",
       "      <td>686.084094</td>\n",
       "      <td>71.420977</td>\n",
       "      <td>395.635057</td>\n",
       "      <td>336.448600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-31</th>\n",
       "      <td>2.340500</td>\n",
       "      <td>19.003671</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.450011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>224.046000</td>\n",
       "      <td>808.898002</td>\n",
       "      <td>155.628500</td>\n",
       "      <td>17.644900</td>\n",
       "      <td>804.451613</td>\n",
       "      <td>41.214470</td>\n",
       "      <td>677.913384</td>\n",
       "      <td>46.024763</td>\n",
       "      <td>389.781879</td>\n",
       "      <td>336.448600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>19.941325</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.241397</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>218.414306</td>\n",
       "      <td>803.050799</td>\n",
       "      <td>152.045799</td>\n",
       "      <td>17.644900</td>\n",
       "      <td>898.104774</td>\n",
       "      <td>40.161730</td>\n",
       "      <td>617.225806</td>\n",
       "      <td>41.399778</td>\n",
       "      <td>376.201778</td>\n",
       "      <td>336.448600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-31</th>\n",
       "      <td>11.599700</td>\n",
       "      <td>129.656301</td>\n",
       "      <td>7.156457</td>\n",
       "      <td>137.000593</td>\n",
       "      <td>138.246937</td>\n",
       "      <td>476.256141</td>\n",
       "      <td>122.557991</td>\n",
       "      <td>24.989672</td>\n",
       "      <td>32.889274</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>210.929346</td>\n",
       "      <td>804.729325</td>\n",
       "      <td>142.301568</td>\n",
       "      <td>17.644900</td>\n",
       "      <td>817.411993</td>\n",
       "      <td>48.494624</td>\n",
       "      <td>723.165126</td>\n",
       "      <td>39.327957</td>\n",
       "      <td>363.244624</td>\n",
       "      <td>336.448600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-30</th>\n",
       "      <td>11.287759</td>\n",
       "      <td>125.163876</td>\n",
       "      <td>8.575442</td>\n",
       "      <td>117.850298</td>\n",
       "      <td>139.451656</td>\n",
       "      <td>475.041116</td>\n",
       "      <td>143.324755</td>\n",
       "      <td>25.390179</td>\n",
       "      <td>39.441722</td>\n",
       "      <td>1.092045</td>\n",
       "      <td>...</td>\n",
       "      <td>174.911296</td>\n",
       "      <td>783.391817</td>\n",
       "      <td>126.040405</td>\n",
       "      <td>17.644900</td>\n",
       "      <td>896.836257</td>\n",
       "      <td>40.444443</td>\n",
       "      <td>800.624814</td>\n",
       "      <td>40.930554</td>\n",
       "      <td>377.012501</td>\n",
       "      <td>336.448600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-31</th>\n",
       "      <td>10.736830</td>\n",
       "      <td>123.986829</td>\n",
       "      <td>9.639141</td>\n",
       "      <td>178.789340</td>\n",
       "      <td>139.940540</td>\n",
       "      <td>487.882589</td>\n",
       "      <td>150.775802</td>\n",
       "      <td>25.434210</td>\n",
       "      <td>39.622161</td>\n",
       "      <td>1.127768</td>\n",
       "      <td>...</td>\n",
       "      <td>167.207706</td>\n",
       "      <td>786.945261</td>\n",
       "      <td>128.612758</td>\n",
       "      <td>17.644900</td>\n",
       "      <td>503.797499</td>\n",
       "      <td>44.838710</td>\n",
       "      <td>886.255794</td>\n",
       "      <td>41.592743</td>\n",
       "      <td>391.650536</td>\n",
       "      <td>336.448600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>10.972681</td>\n",
       "      <td>127.870981</td>\n",
       "      <td>10.140569</td>\n",
       "      <td>168.179443</td>\n",
       "      <td>144.218140</td>\n",
       "      <td>494.057570</td>\n",
       "      <td>143.693933</td>\n",
       "      <td>26.321881</td>\n",
       "      <td>39.781759</td>\n",
       "      <td>1.129527</td>\n",
       "      <td>...</td>\n",
       "      <td>186.641667</td>\n",
       "      <td>801.952554</td>\n",
       "      <td>141.880847</td>\n",
       "      <td>17.644900</td>\n",
       "      <td>489.244342</td>\n",
       "      <td>48.252689</td>\n",
       "      <td>915.283183</td>\n",
       "      <td>43.240592</td>\n",
       "      <td>396.638440</td>\n",
       "      <td>336.448600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-30</th>\n",
       "      <td>11.775854</td>\n",
       "      <td>137.756722</td>\n",
       "      <td>9.042064</td>\n",
       "      <td>173.854215</td>\n",
       "      <td>152.957938</td>\n",
       "      <td>490.213072</td>\n",
       "      <td>148.202456</td>\n",
       "      <td>27.788972</td>\n",
       "      <td>39.685546</td>\n",
       "      <td>1.150772</td>\n",
       "      <td>...</td>\n",
       "      <td>203.347697</td>\n",
       "      <td>793.950347</td>\n",
       "      <td>147.136400</td>\n",
       "      <td>17.484491</td>\n",
       "      <td>497.076298</td>\n",
       "      <td>54.388891</td>\n",
       "      <td>891.225066</td>\n",
       "      <td>42.625002</td>\n",
       "      <td>398.383334</td>\n",
       "      <td>780.864357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-31</th>\n",
       "      <td>12.766657</td>\n",
       "      <td>138.895081</td>\n",
       "      <td>7.010311</td>\n",
       "      <td>155.178977</td>\n",
       "      <td>160.938526</td>\n",
       "      <td>459.336894</td>\n",
       "      <td>127.082199</td>\n",
       "      <td>28.194013</td>\n",
       "      <td>35.069456</td>\n",
       "      <td>0.904993</td>\n",
       "      <td>...</td>\n",
       "      <td>200.588141</td>\n",
       "      <td>780.949828</td>\n",
       "      <td>136.101687</td>\n",
       "      <td>NaN</td>\n",
       "      <td>452.048637</td>\n",
       "      <td>50.510753</td>\n",
       "      <td>738.368432</td>\n",
       "      <td>40.720430</td>\n",
       "      <td>382.227151</td>\n",
       "      <td>976.653696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-30</th>\n",
       "      <td>12.035072</td>\n",
       "      <td>137.947454</td>\n",
       "      <td>5.379124</td>\n",
       "      <td>132.584806</td>\n",
       "      <td>171.593024</td>\n",
       "      <td>479.903720</td>\n",
       "      <td>104.857135</td>\n",
       "      <td>28.168213</td>\n",
       "      <td>26.020133</td>\n",
       "      <td>0.733904</td>\n",
       "      <td>...</td>\n",
       "      <td>200.018241</td>\n",
       "      <td>776.196516</td>\n",
       "      <td>133.154803</td>\n",
       "      <td>38.461519</td>\n",
       "      <td>369.750764</td>\n",
       "      <td>44.638888</td>\n",
       "      <td>730.367858</td>\n",
       "      <td>47.362501</td>\n",
       "      <td>398.766667</td>\n",
       "      <td>931.726906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-31</th>\n",
       "      <td>11.231440</td>\n",
       "      <td>123.760980</td>\n",
       "      <td>5.116499</td>\n",
       "      <td>124.842155</td>\n",
       "      <td>175.251291</td>\n",
       "      <td>475.943998</td>\n",
       "      <td>96.308873</td>\n",
       "      <td>27.945042</td>\n",
       "      <td>28.582395</td>\n",
       "      <td>0.714331</td>\n",
       "      <td>...</td>\n",
       "      <td>173.310795</td>\n",
       "      <td>747.903771</td>\n",
       "      <td>127.666252</td>\n",
       "      <td>NaN</td>\n",
       "      <td>309.412934</td>\n",
       "      <td>42.741937</td>\n",
       "      <td>705.673115</td>\n",
       "      <td>44.266130</td>\n",
       "      <td>374.764785</td>\n",
       "      <td>833.935018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-31</th>\n",
       "      <td>11.635391</td>\n",
       "      <td>149.949569</td>\n",
       "      <td>5.371042</td>\n",
       "      <td>131.945783</td>\n",
       "      <td>176.528536</td>\n",
       "      <td>478.523930</td>\n",
       "      <td>98.446896</td>\n",
       "      <td>28.080554</td>\n",
       "      <td>25.673019</td>\n",
       "      <td>0.700493</td>\n",
       "      <td>...</td>\n",
       "      <td>169.367199</td>\n",
       "      <td>751.250139</td>\n",
       "      <td>127.795660</td>\n",
       "      <td>13.513512</td>\n",
       "      <td>316.467728</td>\n",
       "      <td>42.963706</td>\n",
       "      <td>729.372773</td>\n",
       "      <td>47.174055</td>\n",
       "      <td>406.532932</td>\n",
       "      <td>772.908375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-28</th>\n",
       "      <td>11.923796</td>\n",
       "      <td>157.755629</td>\n",
       "      <td>5.411258</td>\n",
       "      <td>130.143976</td>\n",
       "      <td>169.085253</td>\n",
       "      <td>485.052734</td>\n",
       "      <td>108.047713</td>\n",
       "      <td>27.513542</td>\n",
       "      <td>25.997942</td>\n",
       "      <td>0.706632</td>\n",
       "      <td>...</td>\n",
       "      <td>205.282736</td>\n",
       "      <td>769.334216</td>\n",
       "      <td>139.412450</td>\n",
       "      <td>NaN</td>\n",
       "      <td>366.381298</td>\n",
       "      <td>45.178571</td>\n",
       "      <td>707.346344</td>\n",
       "      <td>43.461194</td>\n",
       "      <td>398.784226</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-31</th>\n",
       "      <td>11.983497</td>\n",
       "      <td>138.538577</td>\n",
       "      <td>5.357480</td>\n",
       "      <td>121.803748</td>\n",
       "      <td>165.223685</td>\n",
       "      <td>488.167825</td>\n",
       "      <td>100.605319</td>\n",
       "      <td>27.118557</td>\n",
       "      <td>25.923171</td>\n",
       "      <td>0.691476</td>\n",
       "      <td>...</td>\n",
       "      <td>204.834455</td>\n",
       "      <td>774.472746</td>\n",
       "      <td>142.828499</td>\n",
       "      <td>NaN</td>\n",
       "      <td>380.238185</td>\n",
       "      <td>43.629032</td>\n",
       "      <td>789.818855</td>\n",
       "      <td>40.752688</td>\n",
       "      <td>395.037634</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-30</th>\n",
       "      <td>11.579068</td>\n",
       "      <td>143.632093</td>\n",
       "      <td>5.768383</td>\n",
       "      <td>141.687454</td>\n",
       "      <td>151.429772</td>\n",
       "      <td>489.818412</td>\n",
       "      <td>121.048948</td>\n",
       "      <td>26.327582</td>\n",
       "      <td>29.287650</td>\n",
       "      <td>0.713076</td>\n",
       "      <td>...</td>\n",
       "      <td>194.261169</td>\n",
       "      <td>767.423970</td>\n",
       "      <td>123.305972</td>\n",
       "      <td>NaN</td>\n",
       "      <td>414.292327</td>\n",
       "      <td>40.180556</td>\n",
       "      <td>743.914418</td>\n",
       "      <td>34.777159</td>\n",
       "      <td>381.656945</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-31</th>\n",
       "      <td>10.715215</td>\n",
       "      <td>127.997888</td>\n",
       "      <td>6.549421</td>\n",
       "      <td>143.563833</td>\n",
       "      <td>141.074918</td>\n",
       "      <td>492.944321</td>\n",
       "      <td>120.711008</td>\n",
       "      <td>25.238743</td>\n",
       "      <td>34.802522</td>\n",
       "      <td>0.681943</td>\n",
       "      <td>...</td>\n",
       "      <td>194.029560</td>\n",
       "      <td>767.190342</td>\n",
       "      <td>122.586680</td>\n",
       "      <td>NaN</td>\n",
       "      <td>439.368856</td>\n",
       "      <td>43.346777</td>\n",
       "      <td>852.619125</td>\n",
       "      <td>37.842809</td>\n",
       "      <td>378.669359</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-30</th>\n",
       "      <td>10.060256</td>\n",
       "      <td>126.159101</td>\n",
       "      <td>6.917087</td>\n",
       "      <td>152.304910</td>\n",
       "      <td>144.210053</td>\n",
       "      <td>495.135549</td>\n",
       "      <td>133.367825</td>\n",
       "      <td>25.524815</td>\n",
       "      <td>39.142679</td>\n",
       "      <td>0.725016</td>\n",
       "      <td>...</td>\n",
       "      <td>172.196505</td>\n",
       "      <td>749.726701</td>\n",
       "      <td>112.430126</td>\n",
       "      <td>NaN</td>\n",
       "      <td>400.701936</td>\n",
       "      <td>51.458333</td>\n",
       "      <td>951.107252</td>\n",
       "      <td>40.586351</td>\n",
       "      <td>389.584722</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-31</th>\n",
       "      <td>10.298359</td>\n",
       "      <td>131.740073</td>\n",
       "      <td>7.697593</td>\n",
       "      <td>159.435822</td>\n",
       "      <td>143.388315</td>\n",
       "      <td>500.180662</td>\n",
       "      <td>131.053038</td>\n",
       "      <td>25.763441</td>\n",
       "      <td>40.261855</td>\n",
       "      <td>0.732281</td>\n",
       "      <td>...</td>\n",
       "      <td>150.672385</td>\n",
       "      <td>740.705203</td>\n",
       "      <td>108.188138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>482.065388</td>\n",
       "      <td>52.473118</td>\n",
       "      <td>952.035912</td>\n",
       "      <td>38.547043</td>\n",
       "      <td>383.741588</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-31</th>\n",
       "      <td>9.514902</td>\n",
       "      <td>127.252244</td>\n",
       "      <td>7.955108</td>\n",
       "      <td>159.009073</td>\n",
       "      <td>151.999701</td>\n",
       "      <td>500.565951</td>\n",
       "      <td>132.254748</td>\n",
       "      <td>26.080570</td>\n",
       "      <td>39.956096</td>\n",
       "      <td>0.733371</td>\n",
       "      <td>...</td>\n",
       "      <td>151.427767</td>\n",
       "      <td>744.139471</td>\n",
       "      <td>113.854615</td>\n",
       "      <td>NaN</td>\n",
       "      <td>462.912879</td>\n",
       "      <td>48.091398</td>\n",
       "      <td>918.861406</td>\n",
       "      <td>35.806191</td>\n",
       "      <td>372.227456</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-30</th>\n",
       "      <td>8.388954</td>\n",
       "      <td>137.318097</td>\n",
       "      <td>6.465466</td>\n",
       "      <td>145.133784</td>\n",
       "      <td>148.381001</td>\n",
       "      <td>457.880164</td>\n",
       "      <td>119.672941</td>\n",
       "      <td>27.186215</td>\n",
       "      <td>39.723332</td>\n",
       "      <td>0.703879</td>\n",
       "      <td>...</td>\n",
       "      <td>155.820104</td>\n",
       "      <td>754.815938</td>\n",
       "      <td>120.418715</td>\n",
       "      <td>NaN</td>\n",
       "      <td>358.466819</td>\n",
       "      <td>47.708333</td>\n",
       "      <td>799.608489</td>\n",
       "      <td>34.308762</td>\n",
       "      <td>360.815278</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-31</th>\n",
       "      <td>9.149909</td>\n",
       "      <td>140.954082</td>\n",
       "      <td>6.038709</td>\n",
       "      <td>135.078645</td>\n",
       "      <td>158.835879</td>\n",
       "      <td>497.170001</td>\n",
       "      <td>115.792125</td>\n",
       "      <td>27.920707</td>\n",
       "      <td>35.110323</td>\n",
       "      <td>0.718539</td>\n",
       "      <td>...</td>\n",
       "      <td>158.180385</td>\n",
       "      <td>751.200079</td>\n",
       "      <td>114.155363</td>\n",
       "      <td>NaN</td>\n",
       "      <td>317.589009</td>\n",
       "      <td>46.086957</td>\n",
       "      <td>763.247422</td>\n",
       "      <td>37.909420</td>\n",
       "      <td>358.260870</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-30</th>\n",
       "      <td>9.089337</td>\n",
       "      <td>143.629940</td>\n",
       "      <td>4.634062</td>\n",
       "      <td>109.166067</td>\n",
       "      <td>173.263963</td>\n",
       "      <td>493.674018</td>\n",
       "      <td>95.517502</td>\n",
       "      <td>28.190162</td>\n",
       "      <td>29.448714</td>\n",
       "      <td>0.708538</td>\n",
       "      <td>...</td>\n",
       "      <td>156.319888</td>\n",
       "      <td>753.910348</td>\n",
       "      <td>119.110313</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31</th>\n",
       "      <td>8.422673</td>\n",
       "      <td>134.839461</td>\n",
       "      <td>3.624674</td>\n",
       "      <td>90.814299</td>\n",
       "      <td>172.473474</td>\n",
       "      <td>488.905503</td>\n",
       "      <td>79.206233</td>\n",
       "      <td>27.830905</td>\n",
       "      <td>25.450286</td>\n",
       "      <td>0.649055</td>\n",
       "      <td>...</td>\n",
       "      <td>140.198779</td>\n",
       "      <td>731.592596</td>\n",
       "      <td>121.965535</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24 rows Ã— 1578 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Panther_parking_Lorriane  Panther_lodging_Cora  \\\n",
       "timestamp                                                    \n",
       "2016-01-31                       NaN             21.797071   \n",
       "2016-02-29                  2.952560             22.008425   \n",
       "2016-03-31                  2.340500             19.003671   \n",
       "2016-04-30                       NaN             19.941325   \n",
       "2016-05-31                 11.599700            129.656301   \n",
       "2016-06-30                 11.287759            125.163876   \n",
       "2016-07-31                 10.736830            123.986829   \n",
       "2016-08-31                 10.972681            127.870981   \n",
       "2016-09-30                 11.775854            137.756722   \n",
       "2016-10-31                 12.766657            138.895081   \n",
       "2016-11-30                 12.035072            137.947454   \n",
       "2016-12-31                 11.231440            123.760980   \n",
       "2017-01-31                 11.635391            149.949569   \n",
       "2017-02-28                 11.923796            157.755629   \n",
       "2017-03-31                 11.983497            138.538577   \n",
       "2017-04-30                 11.579068            143.632093   \n",
       "2017-05-31                 10.715215            127.997888   \n",
       "2017-06-30                 10.060256            126.159101   \n",
       "2017-07-31                 10.298359            131.740073   \n",
       "2017-08-31                  9.514902            127.252244   \n",
       "2017-09-30                  8.388954            137.318097   \n",
       "2017-10-31                  9.149909            140.954082   \n",
       "2017-11-30                  9.089337            143.629940   \n",
       "2017-12-31                  8.422673            134.839461   \n",
       "\n",
       "            Panther_office_Hannah  Panther_lodging_Hattie  \\\n",
       "timestamp                                                   \n",
       "2016-01-31                    NaN                     NaN   \n",
       "2016-02-29                    NaN               16.163100   \n",
       "2016-03-31                    NaN                     NaN   \n",
       "2016-04-30                    NaN                     NaN   \n",
       "2016-05-31               7.156457              137.000593   \n",
       "2016-06-30               8.575442              117.850298   \n",
       "2016-07-31               9.639141              178.789340   \n",
       "2016-08-31              10.140569              168.179443   \n",
       "2016-09-30               9.042064              173.854215   \n",
       "2016-10-31               7.010311              155.178977   \n",
       "2016-11-30               5.379124              132.584806   \n",
       "2016-12-31               5.116499              124.842155   \n",
       "2017-01-31               5.371042              131.945783   \n",
       "2017-02-28               5.411258              130.143976   \n",
       "2017-03-31               5.357480              121.803748   \n",
       "2017-04-30               5.768383              141.687454   \n",
       "2017-05-31               6.549421              143.563833   \n",
       "2017-06-30               6.917087              152.304910   \n",
       "2017-07-31               7.697593              159.435822   \n",
       "2017-08-31               7.955108              159.009073   \n",
       "2017-09-30               6.465466              145.133784   \n",
       "2017-10-31               6.038709              135.078645   \n",
       "2017-11-30               4.634062              109.166067   \n",
       "2017-12-31               3.624674               90.814299   \n",
       "\n",
       "            Panther_education_Teofila  Panther_education_Jerome  \\\n",
       "timestamp                                                         \n",
       "2016-01-31                        NaN                 46.809000   \n",
       "2016-02-29                        NaN                       NaN   \n",
       "2016-03-31                        NaN                       NaN   \n",
       "2016-04-30                        NaN                       NaN   \n",
       "2016-05-31                 138.246937                476.256141   \n",
       "2016-06-30                 139.451656                475.041116   \n",
       "2016-07-31                 139.940540                487.882589   \n",
       "2016-08-31                 144.218140                494.057570   \n",
       "2016-09-30                 152.957938                490.213072   \n",
       "2016-10-31                 160.938526                459.336894   \n",
       "2016-11-30                 171.593024                479.903720   \n",
       "2016-12-31                 175.251291                475.943998   \n",
       "2017-01-31                 176.528536                478.523930   \n",
       "2017-02-28                 169.085253                485.052734   \n",
       "2017-03-31                 165.223685                488.167825   \n",
       "2017-04-30                 151.429772                489.818412   \n",
       "2017-05-31                 141.074918                492.944321   \n",
       "2017-06-30                 144.210053                495.135549   \n",
       "2017-07-31                 143.388315                500.180662   \n",
       "2017-08-31                 151.999701                500.565951   \n",
       "2017-09-30                 148.381001                457.880164   \n",
       "2017-10-31                 158.835879                497.170001   \n",
       "2017-11-30                 173.263963                493.674018   \n",
       "2017-12-31                 172.473474                488.905503   \n",
       "\n",
       "            Panther_retail_Felix  Panther_parking_Asia  \\\n",
       "timestamp                                                \n",
       "2016-01-31                   NaN                   NaN   \n",
       "2016-02-29                   NaN                   NaN   \n",
       "2016-03-31                   NaN                   NaN   \n",
       "2016-04-30                   NaN                   NaN   \n",
       "2016-05-31            122.557991             24.989672   \n",
       "2016-06-30            143.324755             25.390179   \n",
       "2016-07-31            150.775802             25.434210   \n",
       "2016-08-31            143.693933             26.321881   \n",
       "2016-09-30            148.202456             27.788972   \n",
       "2016-10-31            127.082199             28.194013   \n",
       "2016-11-30            104.857135             28.168213   \n",
       "2016-12-31             96.308873             27.945042   \n",
       "2017-01-31             98.446896             28.080554   \n",
       "2017-02-28            108.047713             27.513542   \n",
       "2017-03-31            100.605319             27.118557   \n",
       "2017-04-30            121.048948             26.327582   \n",
       "2017-05-31            120.711008             25.238743   \n",
       "2017-06-30            133.367825             25.524815   \n",
       "2017-07-31            131.053038             25.763441   \n",
       "2017-08-31            132.254748             26.080570   \n",
       "2017-09-30            119.672941             27.186215   \n",
       "2017-10-31            115.792125             27.920707   \n",
       "2017-11-30             95.517502             28.190162   \n",
       "2017-12-31             79.206233             27.830905   \n",
       "\n",
       "            Panther_education_Misty  Panther_retail_Gilbert  ...  \\\n",
       "timestamp                                                    ...   \n",
       "2016-01-31                 5.652859                     NaN  ...   \n",
       "2016-02-29                 5.227683                     NaN  ...   \n",
       "2016-03-31                 7.450011                     NaN  ...   \n",
       "2016-04-30                 7.241397                     NaN  ...   \n",
       "2016-05-31                32.889274                     NaN  ...   \n",
       "2016-06-30                39.441722                1.092045  ...   \n",
       "2016-07-31                39.622161                1.127768  ...   \n",
       "2016-08-31                39.781759                1.129527  ...   \n",
       "2016-09-30                39.685546                1.150772  ...   \n",
       "2016-10-31                35.069456                0.904993  ...   \n",
       "2016-11-30                26.020133                0.733904  ...   \n",
       "2016-12-31                28.582395                0.714331  ...   \n",
       "2017-01-31                25.673019                0.700493  ...   \n",
       "2017-02-28                25.997942                0.706632  ...   \n",
       "2017-03-31                25.923171                0.691476  ...   \n",
       "2017-04-30                29.287650                0.713076  ...   \n",
       "2017-05-31                34.802522                0.681943  ...   \n",
       "2017-06-30                39.142679                0.725016  ...   \n",
       "2017-07-31                40.261855                0.732281  ...   \n",
       "2017-08-31                39.956096                0.733371  ...   \n",
       "2017-09-30                39.723332                0.703879  ...   \n",
       "2017-10-31                35.110323                0.718539  ...   \n",
       "2017-11-30                29.448714                0.708538  ...   \n",
       "2017-12-31                25.450286                0.649055  ...   \n",
       "\n",
       "            Cockatoo_public_Caleb  Cockatoo_education_Tyler  \\\n",
       "timestamp                                                     \n",
       "2016-01-31             191.925676                790.725203   \n",
       "2016-02-29             230.411836                813.343694   \n",
       "2016-03-31             224.046000                808.898002   \n",
       "2016-04-30             218.414306                803.050799   \n",
       "2016-05-31             210.929346                804.729325   \n",
       "2016-06-30             174.911296                783.391817   \n",
       "2016-07-31             167.207706                786.945261   \n",
       "2016-08-31             186.641667                801.952554   \n",
       "2016-09-30             203.347697                793.950347   \n",
       "2016-10-31             200.588141                780.949828   \n",
       "2016-11-30             200.018241                776.196516   \n",
       "2016-12-31             173.310795                747.903771   \n",
       "2017-01-31             169.367199                751.250139   \n",
       "2017-02-28             205.282736                769.334216   \n",
       "2017-03-31             204.834455                774.472746   \n",
       "2017-04-30             194.261169                767.423970   \n",
       "2017-05-31             194.029560                767.190342   \n",
       "2017-06-30             172.196505                749.726701   \n",
       "2017-07-31             150.672385                740.705203   \n",
       "2017-08-31             151.427767                744.139471   \n",
       "2017-09-30             155.820104                754.815938   \n",
       "2017-10-31             158.180385                751.200079   \n",
       "2017-11-30             156.319888                753.910348   \n",
       "2017-12-31             140.198779                731.592596   \n",
       "\n",
       "            Cockatoo_public_Shad  Mouse_health_Buddy  Mouse_health_Modesto  \\\n",
       "timestamp                                                                    \n",
       "2016-01-31            143.564155           17.633042            891.485559   \n",
       "2016-02-29            152.451438           17.644900            800.878622   \n",
       "2016-03-31            155.628500           17.644900            804.451613   \n",
       "2016-04-30            152.045799           17.644900            898.104774   \n",
       "2016-05-31            142.301568           17.644900            817.411993   \n",
       "2016-06-30            126.040405           17.644900            896.836257   \n",
       "2016-07-31            128.612758           17.644900            503.797499   \n",
       "2016-08-31            141.880847           17.644900            489.244342   \n",
       "2016-09-30            147.136400           17.484491            497.076298   \n",
       "2016-10-31            136.101687                 NaN            452.048637   \n",
       "2016-11-30            133.154803           38.461519            369.750764   \n",
       "2016-12-31            127.666252                 NaN            309.412934   \n",
       "2017-01-31            127.795660           13.513512            316.467728   \n",
       "2017-02-28            139.412450                 NaN            366.381298   \n",
       "2017-03-31            142.828499                 NaN            380.238185   \n",
       "2017-04-30            123.305972                 NaN            414.292327   \n",
       "2017-05-31            122.586680                 NaN            439.368856   \n",
       "2017-06-30            112.430126                 NaN            400.701936   \n",
       "2017-07-31            108.188138                 NaN            482.065388   \n",
       "2017-08-31            113.854615                 NaN            462.912879   \n",
       "2017-09-30            120.418715                 NaN            358.466819   \n",
       "2017-10-31            114.155363                 NaN            317.589009   \n",
       "2017-11-30            119.110313                 NaN                   NaN   \n",
       "2017-12-31            121.965535                 NaN                   NaN   \n",
       "\n",
       "            Mouse_lodging_Vicente  Mouse_health_Justin  Mouse_health_Ileana  \\\n",
       "timestamp                                                                     \n",
       "2016-01-31              44.260756           636.270732            67.270160   \n",
       "2016-02-29              45.804598           686.084094            71.420977   \n",
       "2016-03-31              41.214470           677.913384            46.024763   \n",
       "2016-04-30              40.161730           617.225806            41.399778   \n",
       "2016-05-31              48.494624           723.165126            39.327957   \n",
       "2016-06-30              40.444443           800.624814            40.930554   \n",
       "2016-07-31              44.838710           886.255794            41.592743   \n",
       "2016-08-31              48.252689           915.283183            43.240592   \n",
       "2016-09-30              54.388891           891.225066            42.625002   \n",
       "2016-10-31              50.510753           738.368432            40.720430   \n",
       "2016-11-30              44.638888           730.367858            47.362501   \n",
       "2016-12-31              42.741937           705.673115            44.266130   \n",
       "2017-01-31              42.963706           729.372773            47.174055   \n",
       "2017-02-28              45.178571           707.346344            43.461194   \n",
       "2017-03-31              43.629032           789.818855            40.752688   \n",
       "2017-04-30              40.180556           743.914418            34.777159   \n",
       "2017-05-31              43.346777           852.619125            37.842809   \n",
       "2017-06-30              51.458333           951.107252            40.586351   \n",
       "2017-07-31              52.473118           952.035912            38.547043   \n",
       "2017-08-31              48.091398           918.861406            35.806191   \n",
       "2017-09-30              47.708333           799.608489            34.308762   \n",
       "2017-10-31              46.086957           763.247422            37.909420   \n",
       "2017-11-30                    NaN                  NaN                  NaN   \n",
       "2017-12-31                    NaN                  NaN                  NaN   \n",
       "\n",
       "            Mouse_health_Estela  Mouse_science_Micheal  \n",
       "timestamp                                               \n",
       "2016-01-31           388.334665             336.222492  \n",
       "2016-02-29           395.635057             336.448600  \n",
       "2016-03-31           389.781879             336.448600  \n",
       "2016-04-30           376.201778             336.448600  \n",
       "2016-05-31           363.244624             336.448600  \n",
       "2016-06-30           377.012501             336.448600  \n",
       "2016-07-31           391.650536             336.448600  \n",
       "2016-08-31           396.638440             336.448600  \n",
       "2016-09-30           398.383334             780.864357  \n",
       "2016-10-31           382.227151             976.653696  \n",
       "2016-11-30           398.766667             931.726906  \n",
       "2016-12-31           374.764785             833.935018  \n",
       "2017-01-31           406.532932             772.908375  \n",
       "2017-02-28           398.784226                    NaN  \n",
       "2017-03-31           395.037634                    NaN  \n",
       "2017-04-30           381.656945                    NaN  \n",
       "2017-05-31           378.669359                    NaN  \n",
       "2017-06-30           389.584722                    NaN  \n",
       "2017-07-31           383.741588                    NaN  \n",
       "2017-08-31           372.227456                    NaN  \n",
       "2017-09-30           360.815278                    NaN  \n",
       "2017-10-31           358.260870                    NaN  \n",
       "2017-11-30                  NaN                    NaN  \n",
       "2017-12-31                  NaN                    NaN  \n",
       "\n",
       "[24 rows x 1578 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.resample('m').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b768c519-4bef-43dc-b07a-52d30fcefbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Also check out: pd.pivot() and pd.unstack() for dealing with making columns into rows, i.e. wide format -> long format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e56c639-85bb-49f6-bc73-9b0888751729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b287efdc-2c52-41f6-8e11-e9d87360f9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gas <class 'pandas.core.frame.DataFrame'>\n",
      "electricity <class 'pandas.core.frame.DataFrame'>\n",
      "solar <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "for k, v in meter_data.items():\n",
    "    print(k, type(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1f8ffc2-8da4-435e-b7a0-4108dafbe3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "months = [\"January\", \"April\", \"July\", \"October\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1461279e-032f-4b95-a6aa-badb72bf4b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "### We can run a code profiler (using Python's cProfile module) to give us a detailed diagnostic report\n",
    "### of which parts of the `clean_data` function are taking the longest.\n",
    "### https://towardsdatascience.com/how-to-profile-your-code-in-python-e70c834fad89\n",
    "\n",
    "\n",
    "# %%prun -s cumulative -q -l 10 -T code_profile\n",
    "# %prun -s cumulative -l 10\n",
    "\n",
    "\n",
    "\n",
    "### When this function is run, the profiler will generate it's output to a file called clean_data.prof\n",
    "final_data = clean_data(meter_data, metadata, median_energy_usage, months)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ac7571",
   "metadata": {},
   "source": [
    "Linear Logistic Regression Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cb6cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_negative_loglikelihood(Y, pYhat):\n",
    "    \"\"\"\n",
    "    Function for computing the mean negative loglikelihood.\n",
    "    \n",
    "    Y is a vector of the true 0/1 labels.\n",
    "    pYhat is a vector of estimated probabilities, where each entry i is p(Y_i=1 | ... )\n",
    "    \"\"\"\n",
    "    # weigh 0 labels 2x as much as 1 labels\n",
    "    weights = []\n",
    "    for outcome in Y:\n",
    "        if outcome == 0:\n",
    "            weights.append(2)\n",
    "        else:\n",
    "            weights.append(1)\n",
    "    \n",
    "    neg_loglikelihood = (Y*np.log(pYhat))+((1-Y)*np.log(1-pYhat))\n",
    "    weighted_nll = np.multiply(weights, neg_loglikelihood)\n",
    "    mean = np.mean(weighted_nll)\n",
    "\n",
    "    return -mean\n",
    "\n",
    "\n",
    "def accuracy(Y, Yhat):\n",
    "    \"\"\"\n",
    "    Function for computing accuracy.\n",
    "    \n",
    "    Y is a vector of the true labels and Yhat is a vector of estimated 0/1 labels\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.sum(Y==Yhat)/len(Y)\n",
    "    \n",
    "def sigmoid(V):\n",
    "    \"\"\"\n",
    "    Function for mapping a vector of floats to probabilities via the sigmoid function\n",
    "    \"\"\"\n",
    "    \n",
    "    return 1/(1+np.exp(-V))\n",
    "\n",
    "\n",
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, learning_rate=0.1, lamda=None):\n",
    "        \"\"\"\n",
    "        Constructor for the class. Learning rate is\n",
    "        any positive number controlling step size of gradient descent.\n",
    "        Lamda is a positive number controlling the strength of regularization.\n",
    "        When None, no penalty is added.\n",
    "        \"\"\"\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lamda = lamda\n",
    "        self.theta = None # theta is initialized once we fit the model\n",
    "    \n",
    "    def _calculate_gradient(self, Xmat, Y, theta_p, h=1e-5):\n",
    "        \"\"\"\n",
    "        Helper function for computing the gradient at a point theta_p.\n",
    "        \"\"\"\n",
    "        l2_regularization = (self.lamda * (np.sum(theta_p**2))) if self.lamda else 0\n",
    "        \n",
    "        # initialize an empty gradient vector\n",
    "        n, d = Xmat.shape\n",
    "\n",
    "        grad_vec = np.zeros(d)\n",
    "        outcome_no_perturb = mean_negative_loglikelihood(Y, sigmoid(np.dot(Xmat,np.transpose(theta_p)))) + l2_regularization\n",
    "\n",
    "        # Take the partial derivative with respect to each feature and update\n",
    "        # the gradient vector\n",
    "        for i in range(len(grad_vec)):\n",
    "            theta_new = theta_p.copy()\n",
    "            theta_new[i] += h\n",
    "            l2_regularization = (self.lamda * (np.sum(theta_new**2))) if self.lamda else 0\n",
    "            outcome_perturb = mean_negative_loglikelihood(Y, sigmoid(np.dot(Xmat,np.transpose(theta_new)))) + l2_regularization\n",
    "            grad_vec[i] = (outcome_perturb - outcome_no_perturb)/h\n",
    "\n",
    "        return grad_vec\n",
    "\n",
    "    def fit(self, Xmat, Y, max_iterations=1000, tolerance=1e-6, verbose=False):\n",
    "        \"\"\"\n",
    "        Fit a logistic regression model using training data Xmat and Y.\n",
    "        \"\"\"\n",
    "        # add a column of ones for the intercept\n",
    "        n, d = Xmat.shape        \n",
    "        \n",
    "        # initialize theta and theta new randomly\n",
    "        theta = np.random.uniform(-1, 1, d)\n",
    "        theta_new = np.random.uniform(-1, 1, d)\n",
    "        iteration = 0\n",
    "\n",
    "        # keep going until convergence\n",
    "        while iteration < max_iterations and np.mean(np.abs(theta_new-theta)) >= tolerance:\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"Iteration\", iteration, \"theta=\", theta)\n",
    "\n",
    "            # gradient descent\n",
    "            theta_new_og = theta_new.copy()\n",
    "            grad_vec = self._calculate_gradient(Xmat, Y, theta)\n",
    "            theta_new = theta - self.learning_rate*grad_vec\n",
    "            iteration += 1\n",
    "            theta = theta_new_og.copy()\n",
    "            \n",
    "        self.theta = theta_new.copy()\n",
    "        \n",
    "    def predict(self, Xmat):\n",
    "        \"\"\"\n",
    "        Predict 0/1 labels for a data matrix Xmat based on the following rule:\n",
    "        if p(Y=1|X) > 0.5 output a label of 1, else output a label of 0\n",
    "        \"\"\"\n",
    "        # vector of predicated values\n",
    "        pYhat = sigmoid(np.dot(Xmat,np.transpose(self.theta)))\n",
    "        \n",
    "        # fill output vector with binary 1/0 labels\n",
    "        output = []\n",
    "        for row in range(len(Xmat)):\n",
    "            if pYhat[row] > 0.5:\n",
    "                output.append(1) \n",
    "            else:\n",
    "                output.append(0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413e78b5",
   "metadata": {},
   "source": [
    "Split the data into training, testing, and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e258725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data():\n",
    "    '''\n",
    "    Process borrowed from HW4, reads in data from a csv, cleans it, standardizes it, and fits a model \n",
    "    '''\n",
    "    np.random.seed(333)\n",
    "    data_clean = final_data.drop(columns=[\"building_id\",\"source_eui_est\"])\n",
    "    data_clean = data_clean.fillna(0)\n",
    "    feature_names = data_clean.drop(columns=[\"isEfficient\"]).columns.tolist()\n",
    "\n",
    "    \n",
    "    # Dataset for the ablation study -- omits energy meter data\n",
    "    data_ablation = data_clean.copy().drop(columns=[\"electricity_jan\",\"electricity_april\",\"electricity_july\",\"electricity_oct\",\"gas_jan\",\"gas_april\",\"gas_july\",\"gas_oct\",\"solar_jan\",\"solar_april\",\"solar_july\", \"solar_oct\"])\n",
    "    feature_names_ablation = data_ablation.drop(columns=[\"isEfficient\"]).columns.tolist()\n",
    "    \n",
    "    ##############################################################\n",
    "    ### FULL DATA SET  ###\n",
    "    Dmat = data_clean.drop(columns=[\"isEfficient\"]).to_numpy()\n",
    "    Y = data_clean[\"isEfficient\"].to_numpy()\n",
    "    \n",
    "    # standardize the data and add column of 1's for intercept (taken from hw1)\n",
    "    Dcont = Dmat[:, 0:18]\n",
    "    mean = Dcont.mean(axis=0)\n",
    "    std = Dcont.std(axis=0)\n",
    "    Dcont = (Dcont - mean)/std\n",
    "    Xmat = np.column_stack((np.ones(len(Dmat)),Dcont[:,0:18], Dmat[:,19:]))\n",
    "    feature_names = [\"intercept\"] + feature_names\n",
    "    \n",
    "    # split data\n",
    "    Xmat_train, Xmat_test, Y_train, Y_test = train_test_split(Xmat, Y, test_size=0.33, random_state=42)\n",
    "    Xmat_train, Xmat_val, Y_train, Y_val = train_test_split(Xmat_train, Y_train, test_size=0.33, random_state=42)\n",
    "    n, d = Xmat_train.shape\n",
    "    \n",
    "    full_data = {\"Xmat_train\": Xmat_train, \"Xmat_val\": Xmat_val, \"Xmat_test\": Xmat_test,\n",
    "            \"Y_train\": Y_train, \"Y_val\": Y_val, \"Y_test\": Y_test}\n",
    "    \n",
    "    ##################################################################\n",
    "    ### ABLATION DATA SET ###\n",
    "    Dmat_ab = data_ablation.drop(columns=[\"isEfficient\"]).to_numpy()\n",
    "    Y_ab = data_ablation[\"isEfficient\"].to_numpy()\n",
    "    \n",
    "    # standardize the data and add column of 1's for intercept (taken from hw1)\n",
    "    Dcont_ab = Dmat_ab[:, 0:18]\n",
    "    mean_ab = Dcont_ab.mean(axis=0)\n",
    "    std_ab = Dcont_ab.std(axis=0)\n",
    "    Dcont_ab = (Dcont_ab - mean)/std\n",
    "    Xmat_ab = np.column_stack((np.ones(len(Dmat_ab)),Dcont_ab[:,0:18], Dmat_ab[:,19:]))\n",
    "    feature_names_ablation = [\"intercept\"] + feature_names_ablation\n",
    "    \n",
    "    # split data\n",
    "    Xmat_train_ab, Xmat_test_ab, Y_train_ab, Y_test_ab = train_test_split(Xmat_ab, Y_ab, test_size=0.33, random_state=42)\n",
    "    Xmat_train_ab, Xmat_val_ab, Y_train_ab, Y_val_ab = train_test_split(Xmat_train_ab, Y_train_ab, test_size=0.33, random_state=42)\n",
    "    n, d = Xmat_train_ab.shape\n",
    "    \n",
    "    ablation_data = {\"Xmat_train\": Xmat_train_ab, \"Xmat_val\": Xmat_val_ab, \"Xmat_test\": Xmat_test_ab,\n",
    "            \"Y_train\": Y_train_ab, \"Y_val\": Y_val_ab, \"Y_test\": Y_test_ab}\n",
    "    \n",
    "    \n",
    "    return feature_names, feature_names_ablation, full_data, ablation_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65bfb07",
   "metadata": {},
   "source": [
    "Run logistic linear regression and neural net: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7886a472",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names, feature_names_ablation, data, ablation_data = split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a054c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_logistic_model():\n",
    "    \"\"\"\n",
    "    Fits logistic linear regression on an aggregated and cleaned building genome dataset\n",
    "    Prints accuracy results\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Logistic Regression ###\n",
    "    model_base = LogisticRegression(learning_rate=0.2, lamda=0.0)\n",
    "    model_base.fit(data[\"Xmat_train\"], data[\"Y_train\"])\n",
    "\n",
    "    Yhat_test_naive = np.ones((data[\"Y_test\"].shape), dtype=int)\n",
    "    val_size = data[\"Y_val\"].shape\n",
    "    train_size = data[\"Y_train\"].shape\n",
    "    Yhat_val_naive = np.ones(val_size, dtype=int)\n",
    "    Yhat_train_naive = np.ones(train_size, dtype=int)\n",
    "    \n",
    "    Yhat_train_base = model_base.predict(data[\"Xmat_train\"])\n",
    "    Yhat_val_base = model_base.predict(data[\"Xmat_val\"])\n",
    "\n",
    "    ### ACCURACY ###\n",
    "    accuracy_naive = accuracy(data[\"Y_val\"], Yhat_val_naive)\n",
    "    accuracy_base = accuracy(data[\"Y_val\"], Yhat_val_base)\n",
    "    accuracy_train_naive = accuracy(data[\"Y_train\"], Yhat_train_naive)\n",
    "    accuracy_train_base = accuracy(data[\"Y_train\"], Yhat_train_base)\n",
    "\n",
    "    print(\"\\nResults:\\n\" + \"-\"*4)\n",
    "    print(\"Training accuracy naive\", accuracy_train_naive)\n",
    "    print(\"Training accuracy no regularization\", accuracy_train_base)\n",
    "    print(\"Validation accuracy naive\", accuracy_naive)\n",
    "    print(\"Validation accuracy no regularization\", accuracy_base)    \n",
    "    \n",
    "    ## PRECISION, RECALL, F1 ##\n",
    "    precision_naive = precision_score(Yhat_val_naive, data[\"Y_val\"])\n",
    "    recall_naive = recall_score(Yhat_val_naive, data[\"Y_val\"])\n",
    "    f1_naive = f1_score(Yhat_val_naive, data[\"Y_val\"])\n",
    "    \n",
    "    precision_naive_train = precision_score(Yhat_train_naive, data[\"Y_train\"])\n",
    "    recall_naive_train = recall_score(Yhat_train_naive, data[\"Y_train\"])\n",
    "    f1_naive_train = f1_score(Yhat_train_naive, data[\"Y_train\"])\n",
    "    \n",
    "    precision_base = precision_score(Yhat_val_base, data[\"Y_val\"])\n",
    "    recall_base = recall_score(Yhat_val_base, data[\"Y_val\"])\n",
    "    f1_base = f1_score(Yhat_val_base, data[\"Y_val\"])\n",
    "    \n",
    "    precision_base_train = precision_score(Yhat_train_base, data[\"Y_train\"])\n",
    "    recall_base_train = recall_score(Yhat_train_base, data[\"Y_train\"])\n",
    "    f1_base_train = f1_score(Yhat_train_base, data[\"Y_train\"])\n",
    "        \n",
    "    print(\"Naive training precision : \", precision_naive_train, \" recall: \", recall_naive_train, \" F1 score: \", f1_naive_train)\n",
    "    print(\"Base training precision : \", precision_base_train, \" recall: \", recall_base_train, \" F1 score: \", f1_base_train)\n",
    "    print(\"Naive validation precision : \", precision_naive, \" recall: \", recall_naive, \" F1 score: \", f1_naive)\n",
    "    print(\"Base validation precision : \", precision_base, \" recall: \", recall_base, \" F1 score: \", f1_base)\n",
    "\n",
    "    # choose best model\n",
    "    best_model = model_base\n",
    "    Yhat_test = best_model.predict(data[\"Xmat_test\"])\n",
    "    print(\"Logistic Regression Test accuracy\", accuracy(data[\"Y_test\"], Yhat_test))\n",
    "    print(\"Precision: \", precision_score(Yhat_test, data[\"Y_test\"]))\n",
    "    print(\"Recall: \", recall_score(Yhat_test, data[\"Y_test\"]))\n",
    "    print(\"F1Score: \", f1_score(Yhat_test, data[\"Y_test\"]))\n",
    "    print(\"-------------------------------------------------\")\n",
    "    print(\"Naive Classifier Test accuracy\", accuracy(data[\"Y_test\"], Yhat_test_naive))\n",
    "    print(\"Precision: \", precision_score(Yhat_test_naive, data[\"Y_test\"]))\n",
    "    print(\"Recall: \", recall_score(Yhat_test_naive, data[\"Y_test\"]))\n",
    "    print(\"F1Score: \", f1_score(Yhat_test_naive, data[\"Y_test\"]))\n",
    "    print(\"Logistic Regression weights\", {feature_names[i]: round(best_model.theta[i], 2) for i in range(len(feature_names)-1)})\n",
    "\n",
    "run_logistic_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f3b7c0",
   "metadata": {},
   "source": [
    "Neural Net model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84390a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_neuralnet_model():\n",
    "    \"\"\"\n",
    "    Fits logistic linear regression on an aggregated and cleaned building genome dataset\n",
    "    Prints accuracy results\n",
    "    \"\"\"\n",
    "\n",
    "    ##############################################\n",
    "    ### Best Neural Net Model ###\n",
    "    ##############################################\n",
    "    model_base = MLPClassifier(batch_size=10, max_iter=2000, hidden_layer_sizes=(512,))\n",
    "    model_base.fit(data[\"Xmat_train\"], data[\"Y_train\"])\n",
    "   \n",
    "    Yhat_train_base = model_base.predict(data[\"Xmat_train\"])\n",
    "    Yhat_val_base = model_base.predict(data[\"Xmat_val\"])\n",
    "    \n",
    "    ## ACCURACY ##\n",
    "    accuracy_base_val = accuracy(data[\"Y_val\"], Yhat_val_base)\n",
    "    accuracy_base_train = accuracy(data[\"Y_train\"], Yhat_train_base)\n",
    "\n",
    "    print(\"\\nResults for base model:\\n\" + \"-\"*4)\n",
    "    \n",
    "    ## PRECISION, RECALL, F1 ##    \n",
    "    precision_base_val = precision_score(Yhat_val_base, data[\"Y_val\"])\n",
    "    recall_base_val = recall_score(Yhat_val_base, data[\"Y_val\"])\n",
    "    f1_base_val = f1_score(Yhat_val_base, data[\"Y_val\"])\n",
    "    \n",
    "    precision_base_train = precision_score(Yhat_train_base, data[\"Y_train\"])\n",
    "    recall_base_train = recall_score(Yhat_train_base, data[\"Y_train\"])\n",
    "    f1_base_train = f1_score(Yhat_train_base, data[\"Y_train\"])\n",
    "    \n",
    "    print(\"Validation accuracy: \", accuracy_base_val, \"precision : \", precision_base_val, \" recall: \", recall_base_val, \" F1 score: \", f1_base_val)\n",
    "    print(\"Training accuracy: \", accuracy_base_train, \"precision : \", precision_base_train, \" recall: \", recall_base_train, \" F1 score: \", f1_base_train)\n",
    "    print(\"\\n------------------------\")\n",
    "    \n",
    "    \n",
    "    # choose best model\n",
    "    best_model = model_base\n",
    "    Yhat_test = best_model.predict(data[\"Xmat_test\"])\n",
    "    print(\"Test accuracy\", accuracy(data[\"Y_test\"], Yhat_test))\n",
    "    print(\"Test precision : \", precision_score(Yhat_test, data[\"Y_test\"]))\n",
    "    print(\"Test recall: \", recall_score(Yhat_test, data[\"Y_test\"]), \" F1 score: \", f1_score(Yhat_test, data[\"Y_test\"]))\n",
    "run_neuralnet_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aba1f7d",
   "metadata": {},
   "source": [
    "Run all models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330175a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_ablation_model():\n",
    "    model_base = MLPClassifier(batch_size=10, max_iter=2000, hidden_layer_sizes=(512,))\n",
    "    model_base.fit(ablation_data[\"Xmat_train\"], ablation_data[\"Y_train\"])\n",
    "   \n",
    "    Yhat_train_base = model_base.predict(ablation_data[\"Xmat_train\"])\n",
    "    Yhat_val_base = model_base.predict(ablation_data[\"Xmat_val\"])\n",
    "    \n",
    "    ## ACCURACY ##\n",
    "    accuracy_base_val = accuracy(ablation_data[\"Y_val\"], Yhat_val_base)\n",
    "    accuracy_base_train = accuracy(ablation_data[\"Y_train\"], Yhat_train_base)\n",
    "\n",
    "    print(\"\\nResults for ablation model:\\n\" + \"-\"*4)\n",
    "    \n",
    "    ## PRECISION, RECALL, F1 ##    \n",
    "    precision_base_val = precision_score(Yhat_val_base, ablation_data[\"Y_val\"])\n",
    "    recall_base_val = recall_score(Yhat_val_base, ablation_data[\"Y_val\"])\n",
    "    f1_base_val = f1_score(Yhat_val_base, ablation_data[\"Y_val\"])\n",
    "    \n",
    "    precision_base_train = precision_score(Yhat_train_base, ablation_data[\"Y_train\"])\n",
    "    recall_base_train = recall_score(Yhat_train_base, ablation_data[\"Y_train\"])\n",
    "    f1_base_train = f1_score(Yhat_train_base, ablation_data[\"Y_train\"])\n",
    "    \n",
    "    print(\"Validation accuracy: \", accuracy_base_val, \"precision : \", precision_base_val, \" recall: \", recall_base_val, \" F1 score: \", f1_base_val)\n",
    "    print(\"Training accuracy: \", accuracy_base_train, \"precision : \", precision_base_train, \" recall: \", recall_base_train, \" F1 score: \", f1_base_train)\n",
    "    print(\"\\n------------------------\")\n",
    "    \n",
    "    # choose best model\n",
    "    best_model = model_base\n",
    "    Yhat_test = best_model.predict(ablation_data[\"Xmat_test\"])\n",
    "    print(\"Test accuracy\", accuracy(ablation_data[\"Y_test\"], Yhat_test))\n",
    "    print(\"Test precision : \", precision_score(Yhat_test, ablation_data[\"Y_test\"]), \" recall: \", recall_score(Yhat_test, ablation_data[\"Y_test\"]), \" F1 score: \", f1_score(Yhat_test, ablation_data[\"Y_test\"]))\n",
    "    \n",
    "run_ablation_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
